{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8085 Assembly Next Word Predictions:\n",
      "[('01h', 1)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "# Load the 8085 n-gram model\n",
    "with open('n_gram_8085.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "n = 3  # n-gram size\n",
    "\n",
    "def predict_next_word(context):\n",
    "    context = context[-n+1:]\n",
    "    predictions = model.context_counts(model.vocab.lookup(context))\n",
    "    return sorted(predictions.items(), key=lambda x: -x[1])[:5]\n",
    "\n",
    "# Example usage\n",
    "print(\"8085 Assembly Next Word Predictions:\")\n",
    "print(predict_next_word(['mov', 'a', ',']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8085 Assembly Next Word Predictions:\n",
      "[('01h', 1)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "# Load the 8085 n-gram model\n",
    "with open('n_gram_8085.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "n = 3  # n-gram size\n",
    "\n",
    "def predict_next_word(context):\n",
    "    context = context[-n+1:]\n",
    "    predictions = model.context_counts(model.vocab.lookup(context))\n",
    "    return sorted(predictions.items(), key=lambda x: -x[1])[:5]\n",
    "\n",
    "# Example usage\n",
    "print(\"8085 Assembly Next Word Predictions:\")\n",
    "print(predict_next_word(['mov', 'a', ',']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8051 Assembly Next Word Predictions:\n",
      "[('b', 12), ('00h', 8), ('19h', 8), ('13h', 8), ('00000001b', 8)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "# Load the 8051 n-gram model\n",
    "with open('n_gram_8051.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "n = 3  # n-gram size\n",
    "\n",
    "def predict_next_word(context):\n",
    "    context = context[-n+1:]\n",
    "    predictions = model.context_counts(model.vocab.lookup(context))\n",
    "    return sorted(predictions.items(), key=lambda x: -x[1])[:5]\n",
    "\n",
    "# Example usage\n",
    "print(\"8051 Assembly Next Word Predictions:\")\n",
    "print(predict_next_word(['mov', 'a', ',']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "def predict_next_word(model, context, n=3):\n",
    "    \"\"\"Generalized prediction function that works with any model\"\"\"\n",
    "    context = context[-n+1:] if len(context) > 0 else []\n",
    "    \n",
    "    if len(context) > 0:\n",
    "        predictions = model.context_counts(model.vocab.lookup(context))\n",
    "        return sorted(predictions.items(), key=lambda x: -x[1])[:5]\n",
    "    else:\n",
    "        # Handle empty context case\n",
    "        return [(\"nop\", 1)]\n",
    "\n",
    "try:\n",
    "    # Get input from command line\n",
    "    if len(sys.argv) < 2:\n",
    "        print(json.dumps({\"success\": False, \"error\": \"No input provided\"}))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Get the raw input\n",
    "    input_arg = sys.argv[1]\n",
    "    \n",
    "    # Check if input is a file path (starts with @)\n",
    "    if input_arg.startswith('@'):\n",
    "        file_path = input_arg[1:]  # Remove the @ symbol\n",
    "        print(f\"Reading input from file: {file_path}\", file=sys.stderr)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                input_arg = f.read()\n",
    "            print(f\"Input from file: {repr(input_arg)}\", file=sys.stderr)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {e}\", file=sys.stderr)\n",
    "            print(json.dumps({\"success\": False, \"error\": f\"Error reading input file: {str(e)}\"}))\n",
    "            sys.exit(1)\n",
    "    \n",
    "    # Debug the raw input\n",
    "    print(f\"Raw input: {input_arg}\", file=sys.stderr)\n",
    "    \n",
    "    # Pre-process input to handle quote issues\n",
    "    # Windows command line often has issues with quotes\n",
    "    input_json = input_arg\n",
    "    \n",
    "    # Debug the processed input\n",
    "    print(f\"Processed input: {input_json}\", file=sys.stderr)\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(input_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\", file=sys.stderr)\n",
    "        print(f\"Input was: {input_json}\", file=sys.stderr)\n",
    "        print(json.dumps({\"success\": False, \"error\": f\"JSON decode error: {str(e)}\"}))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Get model type and context from input\n",
    "    model_type = data.get('model', '8051')  # Default to 8051 if not specified\n",
    "    context = data.get('context', [])\n",
    "    \n",
    "    # Validate model type\n",
    "    supported_models = ['8051', '8085']\n",
    "    if model_type not in supported_models:\n",
    "        print(f\"Unsupported model type: {model_type}. Using default 8051.\", file=sys.stderr)\n",
    "        model_type = '8051'\n",
    "    \n",
    "    print(f\"Parsed data: model={model_type}, context={context}\", file=sys.stderr)\n",
    "    \n",
    "    # Determine the absolute path to the model files\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    model_path = os.path.join(script_dir, f\"../assets/pickles/n_gram_{model_type}.pkl\")\n",
    "    \n",
    "    print(f\"Looking for model at: {model_path}\", file=sys.stderr)\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found at: {model_path}, using mock data\", file=sys.stderr)\n",
    "        # Provide mock predictions based on the model type\n",
    "        if context and context[0] == \"mov\":\n",
    "            if model_type == '8051':\n",
    "                mock_predictions = [(\"a\", 10), (\"b\", 8), (\"r1\", 6), (\"dptr\", 4), (\"c\", 2)]\n",
    "            else:  # 8085\n",
    "                mock_predictions = [(\"a\", 10), (\"b\", 8), (\"h\", 6), (\"l\", 4), (\"m\", 2)]\n",
    "        else:\n",
    "            mock_predictions = [(\"mov\", 12), (\"add\", 8), (\"jmp\", 6), (\"inc\", 4), (\"ret\", 2)]\n",
    "        \n",
    "        print(json.dumps({\"success\": True, \"predictions\": mock_predictions, \"model\": model_type}))\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # If we reach here, try to use the actual model\n",
    "    try:\n",
    "        # Load the appropriate model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        # Make prediction using our generalized function\n",
    "        result = predict_next_word(model, context)\n",
    "        \n",
    "        # Return the result as JSON\n",
    "        print(json.dumps({\"success\": True, \"predictions\": result, \"model\": model_type}))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error using model: {e}\", file=sys.stderr)\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        \n",
    "        # Fall back to mock data based on model type\n",
    "        if context and context[0] == \"mov\":\n",
    "            if model_type == '8051':\n",
    "                mock_predictions = [(\"a\", 10), (\"b\", 8), (\"r1\", 6), (\"dptr\", 4), (\"c\", 2)]\n",
    "            else:  # 8085\n",
    "                mock_predictions = [(\"a\", 10), (\"b\", 8), (\"h\", 6), (\"l\", 4), (\"m\", 2)]\n",
    "        else:\n",
    "            mock_predictions = [(\"mov\", 12), (\"add\", 8), (\"jmp\", 6), (\"inc\", 4), (\"ret\", 2)]\n",
    "        \n",
    "        print(json.dumps({\"success\": True, \"predictions\": mock_predictions, \"model\": model_type}))\n",
    "        \n",
    "except Exception as e:\n",
    "    # Catch all other exceptions\n",
    "    print(f\"Unexpected error: {str(e)}\", file=sys.stderr)\n",
    "    traceback.print_exc(file=sys.stderr)\n",
    "    print(json.dumps({\"success\": False, \"error\": str(e)}))\n",
    "    sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
